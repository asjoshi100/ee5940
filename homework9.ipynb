{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "import swingUp\n",
    "import numpy as np\n",
    "import numpy.random as rnd\n",
    "import torch as pt\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here is code from the previous assignment for convenience. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class nnQ(pt.nn.Module):\n",
    "    \"\"\"\n",
    "    Here is a basic neural network with for representing a policy \n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self,stateDim,numActions,numHiddenUnits,numLayers):\n",
    "        super().__init__()\n",
    "        \n",
    "        InputLayer = [pt.nn.Linear(stateDim+numActions,numHiddenUnits),\n",
    "                      pt.nn.ReLU()]\n",
    "        \n",
    "        HiddenLayers = []\n",
    "        for _ in range(numLayers-1):\n",
    "            HiddenLayers.append(pt.nn.Linear(numHiddenUnits,numHiddenUnits))\n",
    "            HiddenLayers.append(pt.nn.ReLU())\n",
    "            \n",
    "        \n",
    "        OutputLayer = [pt.nn.Linear(numHiddenUnits,1)]\n",
    "        \n",
    "        AllLayers = InputLayer + HiddenLayers + OutputLayer\n",
    "        self.net = pt.nn.Sequential(*AllLayers)\n",
    "        \n",
    "        self.numActions = numActions\n",
    "        \n",
    "    def forward(self,x,a):\n",
    "        x = pt.tensor(x,dtype=pt.float32)\n",
    "\n",
    "        b = pt.nn.functional.one_hot(pt.tensor(a),self.numActions)\n",
    "        \n",
    "        c = b.float().detach()\n",
    "        y = pt.cat([x,c])\n",
    "        \n",
    "        return self.net(y)\n",
    "            \n",
    "class sarsaAgent:\n",
    "    def __init__(self,stateDim,numActions,numHiddenUnits,numLayers,\n",
    "                epsilon=.1,gamma=.9,alpha=.1):\n",
    "        self.Q = nnQ(stateDim,numActions,numHiddenUnits,numLayers)\n",
    "        self.gamma = gamma\n",
    "        self.epsilon = epsilon\n",
    "        self.alpha = alpha\n",
    "        self.numActions = numActions\n",
    "        self.s_last = None\n",
    "        \n",
    "    def action(self,x):\n",
    "        # This is an epsilon greedy selection\n",
    "        if rnd.rand() < self.epsilon:\n",
    "            a = rnd.randint(numActions)\n",
    "        else:\n",
    "            qBest = -np.inf\n",
    "            for aTest in range(self.numActions):\n",
    "                qTest = self.Q(x,aTest).detach().numpy()[0]\n",
    "                if qTest > qBest:\n",
    "                    qBest = qTest\n",
    "                    a = aTest\n",
    "        return a\n",
    "    \n",
    "    def update(self,s,a,r,s_next,done):\n",
    "        \n",
    "        # Compute the TD error, if there is enough data\n",
    "        update = True\n",
    "        if done:\n",
    "            Q_cur = self.Q(s,a).detach().numpy()[0]\n",
    "            delta = r - Q_cur\n",
    "            self.s_last = None\n",
    "            Q_diff = self.Q(s,a)\n",
    "        elif self.s_last is not None:\n",
    "            Q_next = self.Q(s,a).detach().numpy()[0]\n",
    "            Q_cur = self.Q(self.s_last,self.a_last).detach().numpy()[0]\n",
    "            delta = self.r_last + self.gamma * Q_next - Q_cur\n",
    "            Q_diff = self.Q(self.s_last,self.a_last)\n",
    "        else:\n",
    "            update = False\n",
    "            \n",
    "        # Update the parameter via the semi-gradient method\n",
    "        if update:\n",
    "            self.Q.zero_grad()\n",
    "            Q_diff.backward()\n",
    "            for p in self.Q.parameters():\n",
    "                p.data.add_(self.alpha*delta,p.grad.data)\n",
    "                \n",
    "        \n",
    "            \n",
    "            \n",
    "        \n",
    "        if not done:\n",
    "            self.s_last = np.copy(s)\n",
    "            self.a_last = np.copy(a)\n",
    "            self.r_last = np.copy(r)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The simulation is slightly modified from the previous homework. In particular, the episode lengths are restricted to be at most 500. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode: 1 Total Steps: 38 , Ave. Reward: -20.289310762240213 , Episode Length: 38 Max Up-Time: 0\n",
      "Episode: 2 Total Steps: 83 , Ave. Reward: -15.6507661661448 , Episode Length: 45 Max Up-Time: 0\n",
      "Episode: 3 Total Steps: 134 , Ave. Reward: -9.547764235128634 , Episode Length: 51 Max Up-Time: 0\n",
      "Episode: 4 Total Steps: 634 , Ave. Reward: 0.0 , Episode Length: 500 Max Up-Time: 0\n",
      "Episode: 5 Total Steps: 1134 , Ave. Reward: 0.0 , Episode Length: 500 Max Up-Time: 0\n",
      "Episode: 6 Total Steps: 1634 , Ave. Reward: 0.0 , Episode Length: 500 Max Up-Time: 0\n",
      "Episode: 7 Total Steps: 2134 , Ave. Reward: 0.0 , Episode Length: 500 Max Up-Time: 0\n",
      "Episode: 8 Total Steps: 2634 , Ave. Reward: 0.0 , Episode Length: 500 Max Up-Time: 0\n",
      "Episode: 9 Total Steps: 3134 , Ave. Reward: 0.0 , Episode Length: 500 Max Up-Time: 0\n",
      "Episode: 10 Total Steps: 3634 , Ave. Reward: 0.0 , Episode Length: 500 Max Up-Time: 0\n",
      "Episode: 11 Total Steps: 4134 , Ave. Reward: 0.0 , Episode Length: 500 Max Up-Time: 0\n",
      "Episode: 12 Total Steps: 4634 , Ave. Reward: 0.0 , Episode Length: 500 Max Up-Time: 0\n",
      "Episode: 13 Total Steps: 5134 , Ave. Reward: 0.0 , Episode Length: 500 Max Up-Time: 0\n",
      "Episode: 14 Total Steps: 5634 , Ave. Reward: 0.0 , Episode Length: 500 Max Up-Time: 0\n",
      "Episode: 15 Total Steps: 6134 , Ave. Reward: 0.0 , Episode Length: 500 Max Up-Time: 0\n",
      "Episode: 16 Total Steps: 6634 , Ave. Reward: 0.0 , Episode Length: 500 Max Up-Time: 0\n",
      "Episode: 17 Total Steps: 7134 , Ave. Reward: 0.0 , Episode Length: 500 Max Up-Time: 0\n",
      "Episode: 18 Total Steps: 7634 , Ave. Reward: 0.0 , Episode Length: 500 Max Up-Time: 0\n",
      "Episode: 19 Total Steps: 8134 , Ave. Reward: 0.0 , Episode Length: 500 Max Up-Time: 0\n",
      "Episode: 20 Total Steps: 8634 , Ave. Reward: 0.0 , Episode Length: 500 Max Up-Time: 0\n",
      "Episode: 21 Total Steps: 9134 , Ave. Reward: 0.0 , Episode Length: 500 Max Up-Time: 0\n",
      "Episode: 22 Total Steps: 9634 , Ave. Reward: 0.0 , Episode Length: 500 Max Up-Time: 0\n",
      "Episode: 23 Total Steps: 10134 , Ave. Reward: 0.0 , Episode Length: 500 Max Up-Time: 0\n",
      "Episode: 24 Total Steps: 10634 , Ave. Reward: 0.0 , Episode Length: 500 Max Up-Time: 0\n",
      "Episode: 25 Total Steps: 11134 , Ave. Reward: 0.0 , Episode Length: 500 Max Up-Time: 0\n",
      "Episode: 26 Total Steps: 11634 , Ave. Reward: 0.0 , Episode Length: 500 Max Up-Time: 0\n",
      "Episode: 27 Total Steps: 12134 , Ave. Reward: 0.0 , Episode Length: 500 Max Up-Time: 0\n",
      "Episode: 28 Total Steps: 12634 , Ave. Reward: 0.0 , Episode Length: 500 Max Up-Time: 0\n",
      "Episode: 29 Total Steps: 13134 , Ave. Reward: 0.0 , Episode Length: 500 Max Up-Time: 0\n",
      "Episode: 30 Total Steps: 13634 , Ave. Reward: 0.0 , Episode Length: 500 Max Up-Time: 0\n",
      "Episode: 31 Total Steps: 14134 , Ave. Reward: 0.0 , Episode Length: 500 Max Up-Time: 0\n",
      "Episode: 32 Total Steps: 14634 , Ave. Reward: 0.0 , Episode Length: 500 Max Up-Time: 0\n",
      "Episode: 33 Total Steps: 15134 , Ave. Reward: 0.0 , Episode Length: 500 Max Up-Time: 0\n",
      "Episode: 34 Total Steps: 15634 , Ave. Reward: 0.0 , Episode Length: 500 Max Up-Time: 0\n",
      "Episode: 35 Total Steps: 16134 , Ave. Reward: 0.0 , Episode Length: 500 Max Up-Time: 0\n",
      "Episode: 36 Total Steps: 16634 , Ave. Reward: 0.0 , Episode Length: 500 Max Up-Time: 0\n",
      "Episode: 37 Total Steps: 17134 , Ave. Reward: 0.0 , Episode Length: 500 Max Up-Time: 0\n",
      "Episode: 38 Total Steps: 17634 , Ave. Reward: 0.0 , Episode Length: 500 Max Up-Time: 0\n",
      "Episode: 39 Total Steps: 18134 , Ave. Reward: 0.0 , Episode Length: 500 Max Up-Time: 0\n",
      "Episode: 40 Total Steps: 18634 , Ave. Reward: 0.0 , Episode Length: 500 Max Up-Time: 0\n",
      "Episode: 41 Total Steps: 19134 , Ave. Reward: 0.0 , Episode Length: 500 Max Up-Time: 0\n",
      "Episode: 42 Total Steps: 19375 , Ave. Reward: -3.187725049017287 , Episode Length: 241 Max Up-Time: 7\n",
      "Episode: 43 Total Steps: 19875 , Ave. Reward: 0.0 , Episode Length: 500 Max Up-Time: 0\n",
      "Episode: 44 Total Steps: 20375 , Ave. Reward: 0.0 , Episode Length: 500 Max Up-Time: 0\n",
      "Episode: 45 Total Steps: 20875 , Ave. Reward: 0.0 , Episode Length: 500 Max Up-Time: 0\n",
      "Episode: 46 Total Steps: 21375 , Ave. Reward: 0.0 , Episode Length: 500 Max Up-Time: 0\n",
      "Episode: 47 Total Steps: 21589 , Ave. Reward: -5.446512389685985 , Episode Length: 214 Max Up-Time: 8\n",
      "Episode: 48 Total Steps: 21675 , Ave. Reward: -3.4636825201068278 , Episode Length: 86 Max Up-Time: 0\n",
      "Episode: 49 Total Steps: 21731 , Ave. Reward: -7.410639312184271 , Episode Length: 56 Max Up-Time: 0\n",
      "Episode: 50 Total Steps: 22231 , Ave. Reward: 0.0 , Episode Length: 500 Max Up-Time: 0\n",
      "Episode: 51 Total Steps: 22731 , Ave. Reward: 0.0 , Episode Length: 500 Max Up-Time: 0\n",
      "Episode: 52 Total Steps: 23231 , Ave. Reward: 0.0 , Episode Length: 500 Max Up-Time: 0\n",
      "Episode: 53 Total Steps: 23731 , Ave. Reward: 0.0 , Episode Length: 500 Max Up-Time: 0\n",
      "Episode: 54 Total Steps: 24231 , Ave. Reward: 0.0 , Episode Length: 500 Max Up-Time: 0\n",
      "Episode: 55 Total Steps: 24731 , Ave. Reward: 0.0 , Episode Length: 500 Max Up-Time: 0\n",
      "Episode: 56 Total Steps: 25231 , Ave. Reward: 0.0 , Episode Length: 500 Max Up-Time: 0\n",
      "Episode: 57 Total Steps: 25731 , Ave. Reward: 0.0 , Episode Length: 500 Max Up-Time: 0\n",
      "Episode: 58 Total Steps: 26231 , Ave. Reward: 0.0 , Episode Length: 500 Max Up-Time: 0\n",
      "Episode: 59 Total Steps: 26731 , Ave. Reward: 0.0 , Episode Length: 500 Max Up-Time: 0\n",
      "Episode: 60 Total Steps: 27231 , Ave. Reward: 0.0 , Episode Length: 500 Max Up-Time: 0\n",
      "Episode: 61 Total Steps: 27731 , Ave. Reward: 0.0 , Episode Length: 500 Max Up-Time: 0\n",
      "Episode: 62 Total Steps: 28231 , Ave. Reward: 0.0 , Episode Length: 500 Max Up-Time: 0\n",
      "Episode: 63 Total Steps: 28731 , Ave. Reward: 0.0 , Episode Length: 500 Max Up-Time: 0\n",
      "Episode: 64 Total Steps: 29231 , Ave. Reward: 0.0 , Episode Length: 500 Max Up-Time: 0\n",
      "Episode: 65 Total Steps: 29731 , Ave. Reward: 0.0 , Episode Length: 500 Max Up-Time: 0\n",
      "Episode: 66 Total Steps: 30231 , Ave. Reward: 0.0 , Episode Length: 500 Max Up-Time: 0\n",
      "Episode: 67 Total Steps: 30731 , Ave. Reward: 0.0 , Episode Length: 500 Max Up-Time: 0\n",
      "Episode: 68 Total Steps: 31231 , Ave. Reward: 0.0 , Episode Length: 500 Max Up-Time: 0\n",
      "Episode: 69 Total Steps: 31731 , Ave. Reward: 0.0 , Episode Length: 500 Max Up-Time: 0\n",
      "Episode: 70 Total Steps: 32231 , Ave. Reward: 0.0 , Episode Length: 500 Max Up-Time: 0\n",
      "Episode: 71 Total Steps: 32731 , Ave. Reward: 0.0 , Episode Length: 500 Max Up-Time: 0\n",
      "Episode: 72 Total Steps: 33231 , Ave. Reward: 0.0 , Episode Length: 500 Max Up-Time: 0\n",
      "Episode: 73 Total Steps: 33731 , Ave. Reward: 0.0 , Episode Length: 500 Max Up-Time: 0\n",
      "Episode: 74 Total Steps: 34231 , Ave. Reward: 0.0 , Episode Length: 500 Max Up-Time: 0\n",
      "Episode: 75 Total Steps: 34731 , Ave. Reward: 0.0 , Episode Length: 500 Max Up-Time: 0\n",
      "Episode: 76 Total Steps: 35231 , Ave. Reward: 0.0 , Episode Length: 500 Max Up-Time: 0\n",
      "Episode: 77 Total Steps: 35731 , Ave. Reward: 0.0 , Episode Length: 500 Max Up-Time: 0\n",
      "Episode: 78 Total Steps: 36231 , Ave. Reward: 0.0 , Episode Length: 500 Max Up-Time: 0\n",
      "Episode: 79 Total Steps: 36731 , Ave. Reward: 0.0 , Episode Length: 500 Max Up-Time: 0\n",
      "Episode: 80 Total Steps: 37231 , Ave. Reward: 0.0 , Episode Length: 500 Max Up-Time: 0\n",
      "Episode: 81 Total Steps: 37731 , Ave. Reward: 0.0 , Episode Length: 500 Max Up-Time: 0\n",
      "Episode: 82 Total Steps: 38231 , Ave. Reward: 0.0 , Episode Length: 500 Max Up-Time: 0\n",
      "Episode: 83 Total Steps: 38731 , Ave. Reward: 0.0 , Episode Length: 500 Max Up-Time: 0\n",
      "Episode: 84 Total Steps: 39231 , Ave. Reward: 0.0 , Episode Length: 500 Max Up-Time: 0\n",
      "Episode: 85 Total Steps: 39731 , Ave. Reward: 0.0 , Episode Length: 500 Max Up-Time: 0\n",
      "Episode: 86 Total Steps: 40231 , Ave. Reward: 0.0 , Episode Length: 500 Max Up-Time: 0\n",
      "Episode: 87 Total Steps: 40731 , Ave. Reward: 0.0 , Episode Length: 500 Max Up-Time: 0\n",
      "Episode: 88 Total Steps: 41231 , Ave. Reward: 0.0 , Episode Length: 500 Max Up-Time: 0\n",
      "Episode: 89 Total Steps: 41731 , Ave. Reward: 0.0 , Episode Length: 500 Max Up-Time: 0\n",
      "Episode: 90 Total Steps: 42231 , Ave. Reward: 0.0 , Episode Length: 500 Max Up-Time: 0\n",
      "Episode: 91 Total Steps: 42731 , Ave. Reward: 0.005988023952095809 , Episode Length: 500 Max Up-Time: 3\n",
      "Episode: 92 Total Steps: 43231 , Ave. Reward: 0.0 , Episode Length: 500 Max Up-Time: 0\n",
      "Episode: 93 Total Steps: 43731 , Ave. Reward: 0.0 , Episode Length: 500 Max Up-Time: 0\n",
      "Episode: 94 Total Steps: 44231 , Ave. Reward: 0.0 , Episode Length: 500 Max Up-Time: 0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode: 95 Total Steps: 44712 , Ave. Reward: -2.0380231450620445 , Episode Length: 481 Max Up-Time: 14\n",
      "Episode: 96 Total Steps: 44962 , Ave. Reward: -4.108400833547603 , Episode Length: 250 Max Up-Time: 37\n",
      "Episode: 97 Total Steps: 45000 , Ave. Reward: -17.13404181578279 , Episode Length: 38 Max Up-Time: 0\n",
      "Episode: 98 Total Steps: 45066 , Ave. Reward: -2.9493504842557714 , Episode Length: 66 Max Up-Time: 0\n",
      "Episode: 99 Total Steps: 45149 , Ave. Reward: -1.9148583649691278 , Episode Length: 83 Max Up-Time: 0\n",
      "Episode: 100 Total Steps: 45245 , Ave. Reward: -2.1012017088414523 , Episode Length: 96 Max Up-Time: 0\n",
      "Episode: 101 Total Steps: 45362 , Ave. Reward: -0.7539530479312598 , Episode Length: 117 Max Up-Time: 0\n",
      "Episode: 102 Total Steps: 45456 , Ave. Reward: -1.7329773605930514 , Episode Length: 94 Max Up-Time: 0\n",
      "Episode: 103 Total Steps: 45541 , Ave. Reward: -2.4197586217095326 , Episode Length: 85 Max Up-Time: 0\n",
      "Episode: 104 Total Steps: 45723 , Ave. Reward: -0.8046145127120513 , Episode Length: 182 Max Up-Time: 0\n",
      "Episode: 105 Total Steps: 46223 , Ave. Reward: 0.0 , Episode Length: 500 Max Up-Time: 0\n",
      "Episode: 106 Total Steps: 46723 , Ave. Reward: 0.0 , Episode Length: 500 Max Up-Time: 0\n",
      "Episode: 107 Total Steps: 47223 , Ave. Reward: 0.0 , Episode Length: 500 Max Up-Time: 0\n",
      "Episode: 108 Total Steps: 47723 , Ave. Reward: 0.0 , Episode Length: 500 Max Up-Time: 0\n",
      "Episode: 109 Total Steps: 48223 , Ave. Reward: 0.0 , Episode Length: 500 Max Up-Time: 0\n",
      "Episode: 110 Total Steps: 48723 , Ave. Reward: 0.0 , Episode Length: 500 Max Up-Time: 0\n",
      "Episode: 111 Total Steps: 49223 , Ave. Reward: 0.0 , Episode Length: 500 Max Up-Time: 0\n",
      "Episode: 112 Total Steps: 49723 , Ave. Reward: 0.0 , Episode Length: 500 Max Up-Time: 0\n",
      "Episode: 113 Total Steps: 50223 , Ave. Reward: 0.0 , Episode Length: 500 Max Up-Time: 0\n",
      "Episode: 114 Total Steps: 50723 , Ave. Reward: 0.0 , Episode Length: 500 Max Up-Time: 0\n",
      "Episode: 115 Total Steps: 51223 , Ave. Reward: 0.0 , Episode Length: 500 Max Up-Time: 0\n",
      "Episode: 116 Total Steps: 51723 , Ave. Reward: 0.0 , Episode Length: 500 Max Up-Time: 0\n",
      "Episode: 117 Total Steps: 52223 , Ave. Reward: 0.0 , Episode Length: 500 Max Up-Time: 0\n",
      "Episode: 118 Total Steps: 52723 , Ave. Reward: 0.0 , Episode Length: 500 Max Up-Time: 0\n",
      "Episode: 119 Total Steps: 53223 , Ave. Reward: 0.0 , Episode Length: 500 Max Up-Time: 0\n",
      "Episode: 120 Total Steps: 53723 , Ave. Reward: 0.0 , Episode Length: 500 Max Up-Time: 0\n",
      "Episode: 121 Total Steps: 54223 , Ave. Reward: 0.0 , Episode Length: 500 Max Up-Time: 0\n",
      "Episode: 122 Total Steps: 54723 , Ave. Reward: 0.0 , Episode Length: 500 Max Up-Time: 0\n",
      "Episode: 123 Total Steps: 55223 , Ave. Reward: 0.0 , Episode Length: 500 Max Up-Time: 0\n",
      "Episode: 124 Total Steps: 55723 , Ave. Reward: 0.0 , Episode Length: 500 Max Up-Time: 0\n",
      "Episode: 125 Total Steps: 56223 , Ave. Reward: 0.0 , Episode Length: 500 Max Up-Time: 0\n",
      "Episode: 126 Total Steps: 56723 , Ave. Reward: 0.0 , Episode Length: 500 Max Up-Time: 0\n",
      "Episode: 127 Total Steps: 57223 , Ave. Reward: 0.0 , Episode Length: 500 Max Up-Time: 0\n",
      "Episode: 128 Total Steps: 57723 , Ave. Reward: 0.0 , Episode Length: 500 Max Up-Time: 0\n",
      "Episode: 129 Total Steps: 58223 , Ave. Reward: 0.0 , Episode Length: 500 Max Up-Time: 0\n",
      "Episode: 130 Total Steps: 58723 , Ave. Reward: 0.0 , Episode Length: 500 Max Up-Time: 0\n",
      "Episode: 131 Total Steps: 59223 , Ave. Reward: 0.0 , Episode Length: 500 Max Up-Time: 0\n",
      "Episode: 132 Total Steps: 59723 , Ave. Reward: 0.0 , Episode Length: 500 Max Up-Time: 0\n",
      "Episode: 133 Total Steps: 60223 , Ave. Reward: 0.0 , Episode Length: 500 Max Up-Time: 0\n",
      "Episode: 134 Total Steps: 60723 , Ave. Reward: 0.0 , Episode Length: 500 Max Up-Time: 0\n",
      "Episode: 135 Total Steps: 61223 , Ave. Reward: 0.0 , Episode Length: 500 Max Up-Time: 0\n",
      "Episode: 136 Total Steps: 61723 , Ave. Reward: 0.0 , Episode Length: 500 Max Up-Time: 0\n",
      "Episode: 137 Total Steps: 62223 , Ave. Reward: 0.0 , Episode Length: 500 Max Up-Time: 0\n",
      "Episode: 138 Total Steps: 62723 , Ave. Reward: 0.0 , Episode Length: 500 Max Up-Time: 0\n",
      "Episode: 139 Total Steps: 63223 , Ave. Reward: 0.0 , Episode Length: 500 Max Up-Time: 0\n",
      "Episode: 140 Total Steps: 63723 , Ave. Reward: 0.0 , Episode Length: 500 Max Up-Time: 0\n",
      "Episode: 141 Total Steps: 64223 , Ave. Reward: 0.0 , Episode Length: 500 Max Up-Time: 0\n",
      "Episode: 142 Total Steps: 64723 , Ave. Reward: 0.0 , Episode Length: 500 Max Up-Time: 0\n",
      "Episode: 143 Total Steps: 65223 , Ave. Reward: 0.0 , Episode Length: 500 Max Up-Time: 0\n",
      "Episode: 144 Total Steps: 65723 , Ave. Reward: 0.0 , Episode Length: 500 Max Up-Time: 0\n",
      "Episode: 145 Total Steps: 66223 , Ave. Reward: 0.0 , Episode Length: 500 Max Up-Time: 0\n",
      "Episode: 146 Total Steps: 66723 , Ave. Reward: 0.0 , Episode Length: 500 Max Up-Time: 0\n",
      "Episode: 147 Total Steps: 67223 , Ave. Reward: 0.0 , Episode Length: 500 Max Up-Time: 0\n",
      "Episode: 148 Total Steps: 67723 , Ave. Reward: 0.0 , Episode Length: 500 Max Up-Time: 0\n",
      "Episode: 149 Total Steps: 68223 , Ave. Reward: 0.0 , Episode Length: 500 Max Up-Time: 0\n",
      "Episode: 150 Total Steps: 68723 , Ave. Reward: 0.0 , Episode Length: 500 Max Up-Time: 0\n",
      "Episode: 151 Total Steps: 69223 , Ave. Reward: 0.0 , Episode Length: 500 Max Up-Time: 0\n",
      "Episode: 152 Total Steps: 69723 , Ave. Reward: 0.0 , Episode Length: 500 Max Up-Time: 0\n"
     ]
    }
   ],
   "source": [
    "# This is the environment\n",
    "env = swingUp.SwingUpEnv()\n",
    "\n",
    "# For simplicity, we only consider forces of -1 and 1\n",
    "numActions = 2\n",
    "Actions = np.linspace(-1,1,numActions)\n",
    "\n",
    "# This is our learning agent\n",
    "gamma = .95\n",
    "\n",
    "agent = sarsaAgent(5,numActions,20,2,epsilon=5e-2,gamma=gamma,alpha=1e-4)\n",
    "maxSteps = 2e5\n",
    "\n",
    "# This is a helper to deal with the fact that x[2] is actually an angle\n",
    "x_to_y = lambda x : np.array([x[0],x[1],np.cos(x[2]),np.sin(x[2]),x[3]])\n",
    "\n",
    "R = []\n",
    "UpTime = []\n",
    "\n",
    "step = 0\n",
    "ep = 0\n",
    "maxLen = 500\n",
    "while step < maxSteps:\n",
    "    ep += 1\n",
    "    x = env.reset()\n",
    "    C = 0.\n",
    "    \n",
    "    done = False\n",
    "    t = 1\n",
    "    while not done:\n",
    "        t += 1\n",
    "        step += 1\n",
    "        y = x_to_y(x)\n",
    "        a = agent.action(y)\n",
    "        u = Actions[a:a+1]\n",
    "        env.render()\n",
    "        x_next,c,done,info = env.step(u)\n",
    "        \n",
    "        max_up_time = info['max_up_time']\n",
    "        y_next = x_to_y(x_next)\n",
    "\n",
    "        C += (1./t)*(c-C)\n",
    "        agent.update(y,a,c,y_next,done)\n",
    "        x = np.copy(x_next)\n",
    "        if done:\n",
    "            break\n",
    "            \n",
    "        if step >= maxSteps:\n",
    "            break\n",
    "            \n",
    "        if t > maxLen:\n",
    "            agent.s_last = None\n",
    "            break\n",
    "            \n",
    "        \n",
    "        R.append(C)\n",
    "    UpTime.append(max_up_time)\n",
    "    #print('t:',ep+1,', R:',C,', L:',t-1,', G:',G,', Q:', Q_est, 'U:', max_up_time)\n",
    "    print('Episode:',ep,'Total Steps:',step,', Ave. Reward:',C,', Episode Length:',t-1, 'Max Up-Time:', max_up_time)\n",
    "env.close()\n",
    "\n",
    "plt.plot(UpTime)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Question \n",
    "\n",
    "Implement deep Q-learning as described in the paper here:\n",
    "\n",
    "https://daiwk.github.io/assets/dqn.pdf\n",
    "\n",
    "In this paper, we have the states, and so there is no need to do the pre-processing described there.\n",
    "\n",
    "In my tests on this problem, it works substantially better than the SARSA  implementation \n",
    "with the following design choices:\n",
    "* Use the same Q-network architecture as used  in the SARSA algorithm\n",
    "* Same step size, discount factor, and learning rate as above\n",
    "* Mini-batch size of 20\n",
    "* Update the target network every 100 steps\n",
    "\n",
    "The deep Q-learning method can be implemented via a modification of the SARSA code above.\n",
    "\n",
    "You could probably make it work even better with further tuning.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Implement this code below and test it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
