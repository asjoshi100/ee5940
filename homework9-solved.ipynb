{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "import swingUp\n",
    "import numpy as np\n",
    "import numpy.random as rnd\n",
    "import torch as pt\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here is code from the previous assignment for convenience. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class nnQ(pt.nn.Module):\n",
    "    \"\"\"\n",
    "    Here is a basic neural network with for representing a policy \n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self,stateDim,numActions,numHiddenUnits,numLayers):\n",
    "        super().__init__()\n",
    "        \n",
    "        InputLayer = [pt.nn.Linear(stateDim+numActions,numHiddenUnits),\n",
    "                      pt.nn.ReLU()]\n",
    "        \n",
    "        HiddenLayers = []\n",
    "        for _ in range(numLayers-1):\n",
    "            HiddenLayers.append(pt.nn.Linear(numHiddenUnits,numHiddenUnits))\n",
    "            HiddenLayers.append(pt.nn.ReLU())\n",
    "            \n",
    "        \n",
    "        OutputLayer = [pt.nn.Linear(numHiddenUnits,1)]\n",
    "        \n",
    "        AllLayers = InputLayer + HiddenLayers + OutputLayer\n",
    "        self.net = pt.nn.Sequential(*AllLayers)\n",
    "        \n",
    "        self.numActions = numActions\n",
    "        \n",
    "    def forward(self,x,a):\n",
    "        x = pt.tensor(x,dtype=pt.float32)\n",
    "\n",
    "        b = pt.nn.functional.one_hot(pt.tensor(a),self.numActions)\n",
    "        \n",
    "        c = b.float().detach()\n",
    "        \n",
    "        if len(x.shape) == 1:\n",
    "            y = pt.cat([x,c])\n",
    "        else:\n",
    "            y = pt.cat([x.T,c.T]).T\n",
    "        return self.net(y)\n",
    "        \n",
    "    \n",
    "class deepQagent:\n",
    "    def __init__(self,stateDim,numActions,numHiddenUnits,numLayers,epsilon=.1,gamma=.9,alpha=.1,\n",
    "                c = 100,batch_size=20):\n",
    "        self.Q = nnQ(stateDim,numActions,numHiddenUnits,numLayers)\n",
    "        self.Q_target = nnQ(stateDim,numActions,numHiddenUnits,numLayers)\n",
    "        \n",
    "        self.alpha = alpha\n",
    "        self.gamma = gamma\n",
    "        self.epsilon = epsilon\n",
    "        self.numActions = numActions\n",
    "        \n",
    "        self.S = []\n",
    "        self.A = []\n",
    "        self.S_next = []\n",
    "        self.R = []\n",
    "        self.Done = []\n",
    "        self.batch_size = batch_size\n",
    "        self.c = c\n",
    "        self.optimizer = pt.optim.SGD(self.Q.parameters(),lr=alpha)\n",
    "        self.counter = 0\n",
    "    \n",
    "    def action(self,x):\n",
    "        # This is an epsilon greedy selection\n",
    "        if rnd.rand() < self.epsilon:\n",
    "            a = rnd.randint(numActions)\n",
    "        else:\n",
    "            qBest = -np.inf\n",
    "            for aTest in range(self.numActions):\n",
    "                qTest = self.Q(x,aTest).detach().numpy()[0]\n",
    "                if qTest > qBest:\n",
    "                    qBest = qTest\n",
    "                    a = aTest\n",
    "        return a\n",
    "    \n",
    "    def update(self,s,a,r,s_next,done):\n",
    "        self.counter += 1\n",
    "        \n",
    "        self.S.append(s)\n",
    "        self.A.append(a)\n",
    "        self.R.append(r)\n",
    "        self.S_next.append(s_next)\n",
    "        self.Done.append(done)\n",
    "\n",
    "        B_ind = np.array(rnd.choice(len(self.S),size=self.batch_size),dtype=int)\n",
    "\n",
    "        S = np.array([self.S[j] for j in B_ind])\n",
    "        A = np.array([self.A[j] for j in B_ind])\n",
    "        R = pt.tensor(np.array([self.R[j] for j in B_ind]))\n",
    "        S_next = np.array([self.S_next[j] for j in B_ind])\n",
    "        Done = np.array([self.Done[j] for j in B_ind])\n",
    "        \n",
    "        Q_cur = self.Q(S,A).squeeze()\n",
    "\n",
    "        #B_ind = [-1]\n",
    "\n",
    "        AllActions = np.arange(self.numActions)\n",
    "        Q_next = []\n",
    "        for s_next_j,done_j in zip(S_next,Done):\n",
    "            if done_j:\n",
    "                Q_next.append(0.)\n",
    "            else:\n",
    "                Q_all = self.Q_target(np.tile(s_next_j,(self.numActions,1)),AllActions)\n",
    "                Q_next.append(pt.max(Q_all).detach().numpy())\n",
    "        Q_next = pt.tensor(np.array(Q_next),dtype=pt.float)\n",
    "        loss = .5*pt.mean((R+self.gamma * Q_next-Q_cur)**2)\n",
    "        \n",
    "        self.optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        self.optimizer.step()\n",
    "        \n",
    "      \n",
    "        if (self.counter % self.c) == 0:\n",
    "            for p, p_target in zip(self.Q.parameters(),self.Q_target.parameters()):\n",
    "                p_target.data = p.data.clone().detach()\n",
    "                \n",
    "            \n",
    "class sarsaAgent:\n",
    "    def __init__(self,stateDim,numActions,numHiddenUnits,numLayers,\n",
    "                epsilon=.1,gamma=.9,alpha=.1):\n",
    "        self.Q = nnQ(stateDim,numActions,numHiddenUnits,numLayers)\n",
    "        self.gamma = gamma\n",
    "        self.epsilon = epsilon\n",
    "        self.alpha = alpha\n",
    "        self.numActions = numActions\n",
    "        self.s_last = None\n",
    "        \n",
    "    def action(self,x):\n",
    "        # This is an epsilon greedy selection\n",
    "        if rnd.rand() < self.epsilon:\n",
    "            a = rnd.randint(numActions)\n",
    "        else:\n",
    "            qBest = -np.inf\n",
    "            for aTest in range(self.numActions):\n",
    "                qTest = self.Q(x,aTest).detach().numpy()[0]\n",
    "                if qTest > qBest:\n",
    "                    qBest = qTest\n",
    "                    a = aTest\n",
    "        return a\n",
    "    \n",
    "    def update(self,s,a,r,s_next,done):\n",
    "        \n",
    "        # Compute the TD error, if there is enough data\n",
    "        update = True\n",
    "        if done:\n",
    "            Q_cur = self.Q(s,a).detach().numpy()[0]\n",
    "            delta = r - Q_cur\n",
    "            self.s_last = None\n",
    "            Q_diff = self.Q(s,a)\n",
    "        elif self.s_last is not None:\n",
    "            Q_next = self.Q(s,a).detach().numpy()[0]\n",
    "            Q_cur = self.Q(self.s_last,self.a_last).detach().numpy()[0]\n",
    "            delta = self.r_last + self.gamma * Q_next - Q_cur\n",
    "            Q_diff = self.Q(self.s_last,self.a_last)\n",
    "        else:\n",
    "            update = False\n",
    "            \n",
    "        # Update the parameter via the semi-gradient method\n",
    "        if update:\n",
    "            self.Q.zero_grad()\n",
    "            Q_diff.backward()\n",
    "            for p in self.Q.parameters():\n",
    "                p.data.add_(self.alpha*delta,p.grad.data)\n",
    "                \n",
    "        if not done:\n",
    "            self.s_last = np.copy(s)\n",
    "            self.a_last = np.copy(a)\n",
    "            self.r_last = np.copy(r)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The simulation is slightly modified from the previous homework. In particular, the episode lengths are restricted to be at most 500. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode: 1 Total Steps: 88 , Ave. Reward: -3.725157898941912 , Episode Length: 88 Max Up-Time: 0\n",
      "Episode: 2 Total Steps: 140 , Ave. Reward: -7.7086318013417685 , Episode Length: 52 Max Up-Time: 0\n",
      "Episode: 3 Total Steps: 251 , Ave. Reward: -7.784751853829386 , Episode Length: 111 Max Up-Time: 0\n",
      "Episode: 4 Total Steps: 398 , Ave. Reward: -1.5982383619105913 , Episode Length: 147 Max Up-Time: 0\n",
      "Episode: 5 Total Steps: 456 , Ave. Reward: -6.498327173796934 , Episode Length: 58 Max Up-Time: 0\n",
      "Episode: 6 Total Steps: 561 , Ave. Reward: -3.757706709450127 , Episode Length: 105 Max Up-Time: 0\n",
      "Episode: 7 Total Steps: 669 , Ave. Reward: -2.3877569831760104 , Episode Length: 108 Max Up-Time: 0\n",
      "Episode: 8 Total Steps: 727 , Ave. Reward: -5.3834299109036206 , Episode Length: 58 Max Up-Time: 0\n",
      "Episode: 9 Total Steps: 929 , Ave. Reward: -1.550960189584597 , Episode Length: 202 Max Up-Time: 0\n",
      "Episode: 10 Total Steps: 1095 , Ave. Reward: -4.888388865670214 , Episode Length: 166 Max Up-Time: 0\n",
      "Episode: 11 Total Steps: 1378 , Ave. Reward: -2.8097970019235365 , Episode Length: 283 Max Up-Time: 0\n",
      "Episode: 12 Total Steps: 1444 , Ave. Reward: -13.365947354951073 , Episode Length: 66 Max Up-Time: 0\n",
      "Episode: 13 Total Steps: 1944 , Ave. Reward: 0.0 , Episode Length: 500 Max Up-Time: 0\n",
      "Episode: 14 Total Steps: 2202 , Ave. Reward: -3.471409671664174 , Episode Length: 258 Max Up-Time: 14\n",
      "Episode: 15 Total Steps: 2247 , Ave. Reward: -15.252198063557204 , Episode Length: 45 Max Up-Time: 11\n",
      "Episode: 16 Total Steps: 2364 , Ave. Reward: -7.446343668129543 , Episode Length: 117 Max Up-Time: 15\n",
      "Episode: 17 Total Steps: 2415 , Ave. Reward: -12.050433037655598 , Episode Length: 51 Max Up-Time: 10\n",
      "Episode: 18 Total Steps: 2618 , Ave. Reward: -4.727321933549934 , Episode Length: 203 Max Up-Time: 11\n",
      "Episode: 19 Total Steps: 2737 , Ave. Reward: -1.6715306153115368 , Episode Length: 119 Max Up-Time: 0\n",
      "Episode: 20 Total Steps: 2784 , Ave. Reward: -15.560827705240449 , Episode Length: 47 Max Up-Time: 3\n",
      "Episode: 21 Total Steps: 2844 , Ave. Reward: -13.218324541102598 , Episode Length: 60 Max Up-Time: 0\n",
      "Episode: 22 Total Steps: 2974 , Ave. Reward: -1.4833247638881268 , Episode Length: 130 Max Up-Time: 0\n",
      "Episode: 23 Total Steps: 3046 , Ave. Reward: -11.281424039716846 , Episode Length: 72 Max Up-Time: 0\n",
      "Episode: 24 Total Steps: 3159 , Ave. Reward: -1.9916052062475407 , Episode Length: 113 Max Up-Time: 0\n",
      "Episode: 25 Total Steps: 3227 , Ave. Reward: -8.089670711212513 , Episode Length: 68 Max Up-Time: 0\n",
      "Episode: 26 Total Steps: 3284 , Ave. Reward: -14.25502316295598 , Episode Length: 57 Max Up-Time: 19\n",
      "Episode: 27 Total Steps: 3385 , Ave. Reward: -2.1194935388410574 , Episode Length: 101 Max Up-Time: 0\n",
      "Episode: 28 Total Steps: 3419 , Ave. Reward: -15.908519850801593 , Episode Length: 34 Max Up-Time: 0\n",
      "Episode: 29 Total Steps: 3458 , Ave. Reward: -12.544393441482852 , Episode Length: 39 Max Up-Time: 0\n",
      "Episode: 30 Total Steps: 3514 , Ave. Reward: -13.467147283893647 , Episode Length: 56 Max Up-Time: 20\n",
      "Episode: 31 Total Steps: 3621 , Ave. Reward: -5.805370391469486 , Episode Length: 107 Max Up-Time: 0\n",
      "Episode: 32 Total Steps: 3687 , Ave. Reward: -7.808530067537782 , Episode Length: 66 Max Up-Time: 0\n",
      "Episode: 33 Total Steps: 3742 , Ave. Reward: -11.089205868578253 , Episode Length: 55 Max Up-Time: 0\n",
      "Episode: 34 Total Steps: 3789 , Ave. Reward: -12.81998929380859 , Episode Length: 47 Max Up-Time: 0\n",
      "Episode: 35 Total Steps: 3923 , Ave. Reward: -2.9699705175581643 , Episode Length: 134 Max Up-Time: 0\n",
      "Episode: 36 Total Steps: 3963 , Ave. Reward: -10.403385879192145 , Episode Length: 40 Max Up-Time: 0\n",
      "Episode: 37 Total Steps: 4066 , Ave. Reward: -10.099197165105194 , Episode Length: 103 Max Up-Time: 38\n",
      "Episode: 38 Total Steps: 4185 , Ave. Reward: -14.207411104125603 , Episode Length: 119 Max Up-Time: 40\n",
      "Episode: 39 Total Steps: 4351 , Ave. Reward: -10.149570802544034 , Episode Length: 166 Max Up-Time: 34\n",
      "Episode: 40 Total Steps: 4469 , Ave. Reward: -8.823177984785971 , Episode Length: 118 Max Up-Time: 54\n",
      "Episode: 41 Total Steps: 4605 , Ave. Reward: -12.278198870507183 , Episode Length: 136 Max Up-Time: 35\n",
      "Episode: 42 Total Steps: 4704 , Ave. Reward: -9.486896422890135 , Episode Length: 99 Max Up-Time: 43\n",
      "Episode: 43 Total Steps: 4830 , Ave. Reward: -7.615470816996277 , Episode Length: 126 Max Up-Time: 65\n",
      "Episode: 44 Total Steps: 4920 , Ave. Reward: -13.130653353436967 , Episode Length: 90 Max Up-Time: 40\n",
      "Episode: 45 Total Steps: 4992 , Ave. Reward: -2.168324135070594 , Episode Length: 72 Max Up-Time: 45\n",
      "Episode: 46 Total Steps: 5086 , Ave. Reward: -7.209598012750002 , Episode Length: 94 Max Up-Time: 47\n",
      "Episode: 47 Total Steps: 5176 , Ave. Reward: -8.396757826520231 , Episode Length: 90 Max Up-Time: 40\n",
      "Episode: 48 Total Steps: 5296 , Ave. Reward: -9.294533632730818 , Episode Length: 120 Max Up-Time: 56\n",
      "Episode: 49 Total Steps: 5334 , Ave. Reward: -10.896703494327904 , Episode Length: 38 Max Up-Time: 0\n",
      "Episode: 50 Total Steps: 5438 , Ave. Reward: -12.372906282756427 , Episode Length: 104 Max Up-Time: 49\n",
      "Episode: 51 Total Steps: 5483 , Ave. Reward: -7.044361541737974 , Episode Length: 45 Max Up-Time: 16\n",
      "Episode: 52 Total Steps: 5550 , Ave. Reward: -3.847487564827758 , Episode Length: 67 Max Up-Time: 28\n",
      "Episode: 53 Total Steps: 5695 , Ave. Reward: -6.9988981124487 , Episode Length: 145 Max Up-Time: 79\n",
      "Episode: 54 Total Steps: 5738 , Ave. Reward: -11.567442216497012 , Episode Length: 43 Max Up-Time: 0\n",
      "Episode: 55 Total Steps: 5777 , Ave. Reward: -9.066445256011601 , Episode Length: 39 Max Up-Time: 12\n",
      "Episode: 56 Total Steps: 5822 , Ave. Reward: -11.02695419910025 , Episode Length: 45 Max Up-Time: 7\n",
      "Episode: 57 Total Steps: 5952 , Ave. Reward: -9.514829193223107 , Episode Length: 130 Max Up-Time: 59\n",
      "Episode: 58 Total Steps: 5990 , Ave. Reward: -12.19856608350074 , Episode Length: 38 Max Up-Time: 0\n",
      "Episode: 59 Total Steps: 6037 , Ave. Reward: -11.870200123294474 , Episode Length: 47 Max Up-Time: 9\n",
      "Episode: 60 Total Steps: 6142 , Ave. Reward: -10.856588911756072 , Episode Length: 105 Max Up-Time: 50\n",
      "Episode: 61 Total Steps: 6185 , Ave. Reward: -10.321700694548475 , Episode Length: 43 Max Up-Time: 10\n",
      "Episode: 62 Total Steps: 6229 , Ave. Reward: -10.714566181045125 , Episode Length: 44 Max Up-Time: 12\n",
      "Episode: 63 Total Steps: 6351 , Ave. Reward: -8.146866892127662 , Episode Length: 122 Max Up-Time: 64\n",
      "Episode: 64 Total Steps: 6395 , Ave. Reward: -14.848296396201874 , Episode Length: 44 Max Up-Time: 0\n",
      "Episode: 65 Total Steps: 6438 , Ave. Reward: -10.195836760784458 , Episode Length: 43 Max Up-Time: 13\n",
      "Episode: 66 Total Steps: 6571 , Ave. Reward: -10.870706767722233 , Episode Length: 133 Max Up-Time: 63\n",
      "Episode: 67 Total Steps: 6613 , Ave. Reward: -13.444438363965116 , Episode Length: 42 Max Up-Time: 11\n",
      "Episode: 68 Total Steps: 6662 , Ave. Reward: -10.14479686989175 , Episode Length: 49 Max Up-Time: 15\n",
      "Episode: 69 Total Steps: 6789 , Ave. Reward: -10.00444143200465 , Episode Length: 127 Max Up-Time: 40\n",
      "Episode: 70 Total Steps: 6833 , Ave. Reward: -15.595747154276074 , Episode Length: 44 Max Up-Time: 0\n",
      "Episode: 71 Total Steps: 6935 , Ave. Reward: -11.447835094140455 , Episode Length: 102 Max Up-Time: 24\n",
      "Episode: 72 Total Steps: 6975 , Ave. Reward: -17.56795342864512 , Episode Length: 40 Max Up-Time: 0\n",
      "Episode: 73 Total Steps: 7013 , Ave. Reward: -14.650435489679372 , Episode Length: 38 Max Up-Time: 9\n",
      "Episode: 74 Total Steps: 7048 , Ave. Reward: -19.578334638737836 , Episode Length: 35 Max Up-Time: 0\n",
      "Episode: 75 Total Steps: 7161 , Ave. Reward: -9.28933919998124 , Episode Length: 113 Max Up-Time: 34\n",
      "Episode: 76 Total Steps: 7200 , Ave. Reward: -16.519555534020963 , Episode Length: 39 Max Up-Time: 9\n",
      "Episode: 77 Total Steps: 7302 , Ave. Reward: -11.65610485928153 , Episode Length: 102 Max Up-Time: 33\n",
      "Episode: 78 Total Steps: 7344 , Ave. Reward: -17.17425545263083 , Episode Length: 42 Max Up-Time: 11\n",
      "Episode: 79 Total Steps: 7384 , Ave. Reward: -15.492516758606707 , Episode Length: 40 Max Up-Time: 0\n",
      "Episode: 80 Total Steps: 7426 , Ave. Reward: -17.732147126016077 , Episode Length: 42 Max Up-Time: 0\n",
      "Episode: 81 Total Steps: 7507 , Ave. Reward: -9.111786213183514 , Episode Length: 81 Max Up-Time: 14\n",
      "Episode: 82 Total Steps: 7552 , Ave. Reward: -16.10500230132666 , Episode Length: 45 Max Up-Time: 0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode: 83 Total Steps: 7652 , Ave. Reward: -8.683970786753418 , Episode Length: 100 Max Up-Time: 16\n",
      "Episode: 84 Total Steps: 7693 , Ave. Reward: -15.401514267491917 , Episode Length: 41 Max Up-Time: 8\n",
      "Episode: 85 Total Steps: 7730 , Ave. Reward: -18.114289277298898 , Episode Length: 37 Max Up-Time: 0\n",
      "Episode: 86 Total Steps: 7772 , Ave. Reward: -19.288661802027057 , Episode Length: 42 Max Up-Time: 0\n",
      "Episode: 87 Total Steps: 7855 , Ave. Reward: -8.41874986249707 , Episode Length: 83 Max Up-Time: 13\n",
      "Episode: 88 Total Steps: 7940 , Ave. Reward: -9.20246605832475 , Episode Length: 85 Max Up-Time: 12\n",
      "Episode: 89 Total Steps: 8021 , Ave. Reward: -5.646702266599762 , Episode Length: 81 Max Up-Time: 16\n",
      "Episode: 90 Total Steps: 8098 , Ave. Reward: -10.090197504535576 , Episode Length: 77 Max Up-Time: 0\n",
      "Episode: 91 Total Steps: 8137 , Ave. Reward: -18.286213516732698 , Episode Length: 39 Max Up-Time: 11\n",
      "Episode: 92 Total Steps: 8239 , Ave. Reward: -6.349398428664509 , Episode Length: 102 Max Up-Time: 16\n",
      "Episode: 93 Total Steps: 8426 , Ave. Reward: -1.8722073087441 , Episode Length: 187 Max Up-Time: 122\n",
      "Episode: 94 Total Steps: 8926 , Ave. Reward: 0.14570858283433136 , Episode Length: 500 Max Up-Time: 36\n",
      "Episode: 95 Total Steps: 9426 , Ave. Reward: 0.0 , Episode Length: 500 Max Up-Time: 0\n",
      "Episode: 96 Total Steps: 9926 , Ave. Reward: 0.0 , Episode Length: 500 Max Up-Time: 0\n",
      "Episode: 97 Total Steps: 10426 , Ave. Reward: 0.0 , Episode Length: 500 Max Up-Time: 0\n",
      "Episode: 98 Total Steps: 10926 , Ave. Reward: 0.13173652694610777 , Episode Length: 500 Max Up-Time: 35\n",
      "Episode: 99 Total Steps: 11426 , Ave. Reward: 0.0 , Episode Length: 500 Max Up-Time: 0\n",
      "Episode: 100 Total Steps: 11926 , Ave. Reward: 0.017964071856287425 , Episode Length: 500 Max Up-Time: 9\n",
      "Episode: 101 Total Steps: 12426 , Ave. Reward: 0.08982035928143717 , Episode Length: 500 Max Up-Time: 17\n",
      "Episode: 102 Total Steps: 12926 , Ave. Reward: 0.0 , Episode Length: 500 Max Up-Time: 0\n",
      "Episode: 103 Total Steps: 13426 , Ave. Reward: 0.08183632734530938 , Episode Length: 500 Max Up-Time: 24\n",
      "Episode: 104 Total Steps: 13926 , Ave. Reward: 0.17964071856287417 , Episode Length: 500 Max Up-Time: 31\n",
      "Episode: 105 Total Steps: 14426 , Ave. Reward: 0.15568862275449105 , Episode Length: 500 Max Up-Time: 33\n",
      "Episode: 106 Total Steps: 14926 , Ave. Reward: 0.20758483033932143 , Episode Length: 500 Max Up-Time: 76\n",
      "Episode: 107 Total Steps: 15417 , Ave. Reward: -1.786754791319527 , Episode Length: 491 Max Up-Time: 57\n",
      "Episode: 108 Total Steps: 15912 , Ave. Reward: -1.7634145263695866 , Episode Length: 495 Max Up-Time: 36\n",
      "Episode: 109 Total Steps: 16226 , Ave. Reward: -2.9061285235503194 , Episode Length: 314 Max Up-Time: 56\n",
      "Episode: 110 Total Steps: 16568 , Ave. Reward: -2.665427561643116 , Episode Length: 342 Max Up-Time: 36\n",
      "Episode: 111 Total Steps: 16915 , Ave. Reward: -2.5533078881199525 , Episode Length: 347 Max Up-Time: 48\n",
      "Episode: 112 Total Steps: 17307 , Ave. Reward: -2.298755087544299 , Episode Length: 392 Max Up-Time: 51\n",
      "Episode: 113 Total Steps: 17657 , Ave. Reward: -2.6261514312205674 , Episode Length: 350 Max Up-Time: 77\n",
      "Episode: 114 Total Steps: 17983 , Ave. Reward: -2.877374339577998 , Episode Length: 326 Max Up-Time: 45\n",
      "Episode: 115 Total Steps: 18270 , Ave. Reward: -3.1898244065997874 , Episode Length: 287 Max Up-Time: 55\n",
      "Episode: 116 Total Steps: 18545 , Ave. Reward: -3.2633934714005473 , Episode Length: 275 Max Up-Time: 47\n",
      "Episode: 117 Total Steps: 18801 , Ave. Reward: -3.593811163425171 , Episode Length: 256 Max Up-Time: 40\n",
      "Episode: 118 Total Steps: 19100 , Ave. Reward: -3.030206755244219 , Episode Length: 299 Max Up-Time: 40\n",
      "Episode: 119 Total Steps: 19378 , Ave. Reward: -3.280830523591174 , Episode Length: 278 Max Up-Time: 58\n",
      "Episode: 120 Total Steps: 19633 , Ave. Reward: -3.3960872058767477 , Episode Length: 255 Max Up-Time: 60\n",
      "Episode: 121 Total Steps: 19908 , Ave. Reward: -3.5099917479434617 , Episode Length: 275 Max Up-Time: 34\n",
      "Episode: 122 Total Steps: 20149 , Ave. Reward: -4.074609374307672 , Episode Length: 241 Max Up-Time: 44\n",
      "Episode: 123 Total Steps: 20354 , Ave. Reward: -4.959786512885612 , Episode Length: 205 Max Up-Time: 40\n",
      "Episode: 124 Total Steps: 20609 , Ave. Reward: -3.8334212963038734 , Episode Length: 255 Max Up-Time: 43\n",
      "Episode: 125 Total Steps: 20893 , Ave. Reward: -3.7082353270150707 , Episode Length: 284 Max Up-Time: 30\n",
      "Episode: 126 Total Steps: 21112 , Ave. Reward: -5.167650310662341 , Episode Length: 219 Max Up-Time: 31\n",
      "Episode: 127 Total Steps: 21315 , Ave. Reward: -5.467705187429468 , Episode Length: 203 Max Up-Time: 29\n",
      "Episode: 128 Total Steps: 21492 , Ave. Reward: -6.278102985089454 , Episode Length: 177 Max Up-Time: 29\n",
      "Episode: 129 Total Steps: 21679 , Ave. Reward: -5.2697044312199015 , Episode Length: 187 Max Up-Time: 52\n",
      "Episode: 130 Total Steps: 21878 , Ave. Reward: -5.1014559998653475 , Episode Length: 199 Max Up-Time: 36\n",
      "Episode: 131 Total Steps: 22127 , Ave. Reward: -3.9933934695117568 , Episode Length: 249 Max Up-Time: 49\n",
      "Episode: 132 Total Steps: 22332 , Ave. Reward: -5.247581281171329 , Episode Length: 205 Max Up-Time: 41\n",
      "Episode: 133 Total Steps: 22525 , Ave. Reward: -5.364278319555771 , Episode Length: 193 Max Up-Time: 41\n",
      "Episode: 134 Total Steps: 22702 , Ave. Reward: -6.419539264259656 , Episode Length: 177 Max Up-Time: 25\n",
      "Episode: 135 Total Steps: 22889 , Ave. Reward: -5.863366561268666 , Episode Length: 187 Max Up-Time: 25\n",
      "Episode: 136 Total Steps: 23059 , Ave. Reward: -6.770733080771186 , Episode Length: 170 Max Up-Time: 27\n",
      "Episode: 137 Total Steps: 23259 , Ave. Reward: -5.520468978538775 , Episode Length: 200 Max Up-Time: 27\n",
      "Episode: 138 Total Steps: 23431 , Ave. Reward: -5.889190966153554 , Episode Length: 172 Max Up-Time: 51\n",
      "Episode: 139 Total Steps: 23614 , Ave. Reward: -6.1987397887788385 , Episode Length: 183 Max Up-Time: 27\n",
      "Episode: 140 Total Steps: 23832 , Ave. Reward: -4.720072327689131 , Episode Length: 218 Max Up-Time: 55\n",
      "Episode: 141 Total Steps: 24002 , Ave. Reward: -6.54247797672879 , Episode Length: 170 Max Up-Time: 24\n",
      "Episode: 142 Total Steps: 24213 , Ave. Reward: -4.872605167147604 , Episode Length: 211 Max Up-Time: 37\n",
      "Episode: 143 Total Steps: 24713 , Ave. Reward: 0.5089820359281436 , Episode Length: 500 Max Up-Time: 41\n",
      "Episode: 144 Total Steps: 24882 , Ave. Reward: -5.621670523786671 , Episode Length: 169 Max Up-Time: 44\n",
      "Episode: 145 Total Steps: 25236 , Ave. Reward: -2.535341948003187 , Episode Length: 354 Max Up-Time: 70\n",
      "Episode: 146 Total Steps: 25445 , Ave. Reward: -4.330344026651771 , Episode Length: 209 Max Up-Time: 65\n",
      "Episode: 147 Total Steps: 25602 , Ave. Reward: -6.766893385502572 , Episode Length: 157 Max Up-Time: 29\n",
      "Episode: 148 Total Steps: 25763 , Ave. Reward: -5.8345933714327405 , Episode Length: 161 Max Up-Time: 44\n",
      "Episode: 149 Total Steps: 25967 , Ave. Reward: -5.116151533368184 , Episode Length: 204 Max Up-Time: 33\n",
      "Episode: 150 Total Steps: 26122 , Ave. Reward: -5.923418193348249 , Episode Length: 155 Max Up-Time: 32\n",
      "Episode: 151 Total Steps: 26379 , Ave. Reward: -3.6372134930983484 , Episode Length: 257 Max Up-Time: 43\n",
      "Episode: 152 Total Steps: 26604 , Ave. Reward: -4.348161769787613 , Episode Length: 225 Max Up-Time: 55\n",
      "Episode: 153 Total Steps: 26810 , Ave. Reward: -4.735443841853037 , Episode Length: 206 Max Up-Time: 48\n",
      "Episode: 154 Total Steps: 27086 , Ave. Reward: -3.341591350264523 , Episode Length: 276 Max Up-Time: 68\n",
      "Episode: 155 Total Steps: 27337 , Ave. Reward: -4.141810378889278 , Episode Length: 251 Max Up-Time: 41\n",
      "Episode: 156 Total Steps: 27594 , Ave. Reward: -3.9668053120458744 , Episode Length: 257 Max Up-Time: 97\n",
      "Episode: 157 Total Steps: 27888 , Ave. Reward: -2.8999491239876853 , Episode Length: 294 Max Up-Time: 149\n",
      "Episode: 158 Total Steps: 28388 , Ave. Reward: 0.5169660678642719 , Episode Length: 500 Max Up-Time: 59\n",
      "Episode: 159 Total Steps: 28587 , Ave. Reward: -4.748623628887315 , Episode Length: 199 Max Up-Time: 49\n",
      "Episode: 160 Total Steps: 28734 , Ave. Reward: -6.801285013530812 , Episode Length: 147 Max Up-Time: 33\n",
      "Episode: 161 Total Steps: 28955 , Ave. Reward: -4.727442439883562 , Episode Length: 221 Max Up-Time: 42\n",
      "Episode: 162 Total Steps: 29088 , Ave. Reward: -7.845654919241573 , Episode Length: 133 Max Up-Time: 32\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode: 163 Total Steps: 29588 , Ave. Reward: 0.602794411177645 , Episode Length: 500 Max Up-Time: 85\n",
      "Episode: 164 Total Steps: 29726 , Ave. Reward: -7.396189947098131 , Episode Length: 138 Max Up-Time: 36\n",
      "Episode: 165 Total Steps: 30131 , Ave. Reward: -1.954554989600632 , Episode Length: 405 Max Up-Time: 106\n",
      "Episode: 166 Total Steps: 30274 , Ave. Reward: -6.646594417888127 , Episode Length: 143 Max Up-Time: 33\n",
      "Episode: 167 Total Steps: 30447 , Ave. Reward: -5.547118579140202 , Episode Length: 173 Max Up-Time: 44\n",
      "Episode: 168 Total Steps: 30633 , Ave. Reward: -4.630015079111976 , Episode Length: 186 Max Up-Time: 36\n",
      "Episode: 169 Total Steps: 30773 , Ave. Reward: -7.031014146000479 , Episode Length: 140 Max Up-Time: 28\n",
      "Episode: 170 Total Steps: 30941 , Ave. Reward: -5.406627006049523 , Episode Length: 168 Max Up-Time: 53\n",
      "Episode: 171 Total Steps: 31040 , Ave. Reward: -10.366831247309852 , Episode Length: 99 Max Up-Time: 30\n",
      "Episode: 172 Total Steps: 31208 , Ave. Reward: -5.356260845155223 , Episode Length: 168 Max Up-Time: 48\n",
      "Episode: 173 Total Steps: 31394 , Ave. Reward: -4.705628596801213 , Episode Length: 186 Max Up-Time: 68\n",
      "Episode: 174 Total Steps: 31558 , Ave. Reward: -5.78618539530844 , Episode Length: 164 Max Up-Time: 35\n",
      "Episode: 175 Total Steps: 31724 , Ave. Reward: -5.76416724757558 , Episode Length: 166 Max Up-Time: 37\n",
      "Episode: 176 Total Steps: 31909 , Ave. Reward: -4.812064054635796 , Episode Length: 185 Max Up-Time: 63\n",
      "Episode: 177 Total Steps: 32055 , Ave. Reward: -6.619734605252681 , Episode Length: 146 Max Up-Time: 35\n",
      "Episode: 178 Total Steps: 32222 , Ave. Reward: -5.535601714615483 , Episode Length: 167 Max Up-Time: 44\n",
      "Episode: 179 Total Steps: 32365 , Ave. Reward: -6.392208609310717 , Episode Length: 143 Max Up-Time: 29\n",
      "Episode: 180 Total Steps: 32470 , Ave. Reward: -8.848352293025352 , Episode Length: 105 Max Up-Time: 45\n",
      "Episode: 181 Total Steps: 32624 , Ave. Reward: -5.683580058507448 , Episode Length: 154 Max Up-Time: 51\n",
      "Episode: 182 Total Steps: 32730 , Ave. Reward: -8.988928939094302 , Episode Length: 106 Max Up-Time: 33\n",
      "Episode: 183 Total Steps: 32963 , Ave. Reward: -3.8279760073033646 , Episode Length: 233 Max Up-Time: 66\n",
      "Episode: 184 Total Steps: 33108 , Ave. Reward: -7.145638654674884 , Episode Length: 145 Max Up-Time: 42\n",
      "Episode: 185 Total Steps: 33205 , Ave. Reward: -9.996474019031751 , Episode Length: 97 Max Up-Time: 33\n",
      "Episode: 186 Total Steps: 33440 , Ave. Reward: -3.6415824748519556 , Episode Length: 235 Max Up-Time: 69\n",
      "Episode: 187 Total Steps: 33940 , Ave. Reward: 0.572854291417165 , Episode Length: 500 Max Up-Time: 38\n",
      "Episode: 188 Total Steps: 34411 , Ave. Reward: -1.5667816790655542 , Episode Length: 471 Max Up-Time: 80\n",
      "Episode: 189 Total Steps: 34531 , Ave. Reward: -7.835338322953902 , Episode Length: 120 Max Up-Time: 23\n",
      "Episode: 190 Total Steps: 34757 , Ave. Reward: -3.718149355949787 , Episode Length: 226 Max Up-Time: 84\n",
      "Episode: 191 Total Steps: 34899 , Ave. Reward: -6.560693818661982 , Episode Length: 142 Max Up-Time: 26\n",
      "Episode: 192 Total Steps: 35144 , Ave. Reward: -3.98042641694997 , Episode Length: 245 Max Up-Time: 51\n",
      "Episode: 193 Total Steps: 35286 , Ave. Reward: -6.985851711423742 , Episode Length: 142 Max Up-Time: 29\n",
      "Episode: 194 Total Steps: 35446 , Ave. Reward: -6.042197500352136 , Episode Length: 160 Max Up-Time: 32\n",
      "Episode: 195 Total Steps: 35701 , Ave. Reward: -3.629558694185584 , Episode Length: 255 Max Up-Time: 73\n",
      "Episode: 196 Total Steps: 35855 , Ave. Reward: -6.021484695967626 , Episode Length: 154 Max Up-Time: 39\n",
      "Episode: 197 Total Steps: 36234 , Ave. Reward: -2.0135788204719653 , Episode Length: 379 Max Up-Time: 49\n",
      "Episode: 198 Total Steps: 36418 , Ave. Reward: -4.884795722461568 , Episode Length: 184 Max Up-Time: 38\n",
      "Episode: 199 Total Steps: 36548 , Ave. Reward: -7.387818591333687 , Episode Length: 130 Max Up-Time: 29\n",
      "Episode: 200 Total Steps: 36715 , Ave. Reward: -5.285500750540777 , Episode Length: 167 Max Up-Time: 47\n",
      "Episode: 201 Total Steps: 36956 , Ave. Reward: -3.7459724181456213 , Episode Length: 241 Max Up-Time: 34\n",
      "Episode: 202 Total Steps: 37456 , Ave. Reward: 0.5469061876247506 , Episode Length: 500 Max Up-Time: 39\n",
      "Episode: 203 Total Steps: 37678 , Ave. Reward: -4.091852157171258 , Episode Length: 222 Max Up-Time: 33\n",
      "Episode: 204 Total Steps: 37885 , Ave. Reward: -4.404848696574483 , Episode Length: 207 Max Up-Time: 48\n",
      "Episode: 205 Total Steps: 38190 , Ave. Reward: -2.889702596790968 , Episode Length: 305 Max Up-Time: 51\n",
      "Episode: 206 Total Steps: 38690 , Ave. Reward: 0.604790419161677 , Episode Length: 500 Max Up-Time: 117\n",
      "Episode: 207 Total Steps: 38788 , Ave. Reward: -10.364360457715096 , Episode Length: 98 Max Up-Time: 26\n",
      "Episode: 208 Total Steps: 38994 , Ave. Reward: -4.616963704887891 , Episode Length: 206 Max Up-Time: 39\n",
      "Episode: 209 Total Steps: 39401 , Ave. Reward: -2.161634256212582 , Episode Length: 407 Max Up-Time: 56\n",
      "Episode: 210 Total Steps: 39592 , Ave. Reward: -5.023553364689629 , Episode Length: 191 Max Up-Time: 33\n",
      "Episode: 211 Total Steps: 39693 , Ave. Reward: -10.601794865083935 , Episode Length: 101 Max Up-Time: 24\n",
      "Episode: 212 Total Steps: 40193 , Ave. Reward: 0.5069860279441116 , Episode Length: 500 Max Up-Time: 59\n",
      "Episode: 213 Total Steps: 40693 , Ave. Reward: 0.5369261477045912 , Episode Length: 500 Max Up-Time: 115\n",
      "Episode: 214 Total Steps: 41193 , Ave. Reward: 0.5249500998003997 , Episode Length: 500 Max Up-Time: 73\n",
      "Episode: 215 Total Steps: 41481 , Ave. Reward: -3.2123887553948887 , Episode Length: 288 Max Up-Time: 41\n",
      "Episode: 216 Total Steps: 41981 , Ave. Reward: 0.3772455089820359 , Episode Length: 500 Max Up-Time: 51\n",
      "Episode: 217 Total Steps: 42213 , Ave. Reward: -3.7763386826556786 , Episode Length: 232 Max Up-Time: 33\n",
      "Episode: 218 Total Steps: 42407 , Ave. Reward: -5.419941281778805 , Episode Length: 194 Max Up-Time: 31\n",
      "Episode: 219 Total Steps: 42907 , Ave. Reward: 0.4890219560878245 , Episode Length: 500 Max Up-Time: 75\n",
      "Episode: 220 Total Steps: 43407 , Ave. Reward: 0.4810379241516964 , Episode Length: 500 Max Up-Time: 62\n",
      "Episode: 221 Total Steps: 43907 , Ave. Reward: 0.39520958083832375 , Episode Length: 500 Max Up-Time: 34\n",
      "Episode: 222 Total Steps: 44407 , Ave. Reward: 0.4111776447105789 , Episode Length: 500 Max Up-Time: 83\n",
      "Episode: 223 Total Steps: 44830 , Ave. Reward: -1.9130670676579995 , Episode Length: 423 Max Up-Time: 42\n",
      "Episode: 224 Total Steps: 45233 , Ave. Reward: -2.5219539567642455 , Episode Length: 403 Max Up-Time: 39\n",
      "Episode: 225 Total Steps: 45330 , Ave. Reward: -11.177580918756401 , Episode Length: 97 Max Up-Time: 25\n",
      "Episode: 226 Total Steps: 45467 , Ave. Reward: -7.517421180294195 , Episode Length: 137 Max Up-Time: 29\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m----------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m  Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-3-20493e4f0a61>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     43\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     44\u001b[0m         \u001b[0mC\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;36m1.\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mc\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0mC\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 45\u001b[0;31m         \u001b[0magent\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mc\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0my_next\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mdone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     46\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcopy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_next\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     47\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mdone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-2-3ca1572a4107>\u001b[0m in \u001b[0;36mupdate\u001b[0;34m(self, s, a, r, s_next, done)\u001b[0m\n\u001b[1;32m     88\u001b[0m         \u001b[0mDone\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDone\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mj\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mj\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mB_ind\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     89\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 90\u001b[0;31m         \u001b[0mQ_cur\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mQ\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mS\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mA\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msqueeze\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     91\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     92\u001b[0m         \u001b[0;31m#B_ind = [-1]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    539\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    540\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 541\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    542\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    543\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-2-3ca1572a4107>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x, a)\u001b[0m\n\u001b[1;32m     34\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     35\u001b[0m             \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mT\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mT\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mT\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 36\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnet\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     37\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     38\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    539\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    540\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 541\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    542\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    543\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/torch/nn/modules/container.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m     90\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     91\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mmodule\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_modules\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 92\u001b[0;31m             \u001b[0minput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodule\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     93\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     94\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    539\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    540\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 541\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    542\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    543\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/torch/nn/modules/linear.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m     85\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     86\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 87\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlinear\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbias\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     88\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     89\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mextra_repr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/torch/nn/functional.py\u001b[0m in \u001b[0;36mlinear\u001b[0;34m(input, weight, bias)\u001b[0m\n\u001b[1;32m   1368\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdim\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m2\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mbias\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1369\u001b[0m         \u001b[0;31m# fused op is marginally faster\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1370\u001b[0;31m         \u001b[0mret\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maddmm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbias\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweight\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1371\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1372\u001b[0m         \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmatmul\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# This is the environment\n",
    "env = swingUp.SwingUpEnv()\n",
    "\n",
    "# For simplicity, we only consider forces of -1 and 1\n",
    "numActions = 2\n",
    "Actions = np.linspace(-1,1,numActions)\n",
    "\n",
    "# This is our learning agent\n",
    "gamma = .95\n",
    "agent = deepQagent(5,numActions,50,2,epsilon=1e-1,gamma=gamma,batch_size=50,\n",
    "                   c= 100,alpha=1e-5)\n",
    "\n",
    "#agent = sarsaAgent(5,numActions,20,2,epsilon=5e-2,gamma=gamma,alpha=1e-4)\n",
    "maxSteps = 5e5\n",
    "\n",
    "# This is a helper to deal with the fact that x[2] is actually an angle\n",
    "x_to_y = lambda x : np.array([x[0],x[1],np.cos(x[2]),np.sin(x[2]),x[3]])\n",
    "\n",
    "R = []\n",
    "UpTime = []\n",
    "\n",
    "step = 0\n",
    "ep = 0\n",
    "maxLen = 500\n",
    "while step < maxSteps:\n",
    "    ep += 1\n",
    "    x = env.reset()\n",
    "    C = 0.\n",
    "    \n",
    "    done = False\n",
    "    t = 1\n",
    "    while not done:\n",
    "        t += 1\n",
    "        step += 1\n",
    "        y = x_to_y(x)\n",
    "        a = agent.action(y)\n",
    "        u = Actions[a:a+1]\n",
    "        env.render()\n",
    "        x_next,c,done,info = env.step(u)\n",
    "        \n",
    "        max_up_time = info['max_up_time']\n",
    "        y_next = x_to_y(x_next)\n",
    "\n",
    "        C += (1./t)*(c-C)\n",
    "        agent.update(y,a,c,y_next,done)\n",
    "        x = np.copy(x_next)\n",
    "        if done:\n",
    "            break\n",
    "            \n",
    "        if step >= maxSteps:\n",
    "            break\n",
    "            \n",
    "        if t > maxLen:\n",
    "            agent.s_last = None\n",
    "            break\n",
    "            \n",
    "        \n",
    "        R.append(C)\n",
    "    UpTime.append(max_up_time)\n",
    "    #print('t:',ep+1,', R:',C,', L:',t-1,', G:',G,', Q:', Q_est, 'U:', max_up_time)\n",
    "    print('Episode:',ep,'Total Steps:',step,', Ave. Reward:',C,', Episode Length:',t-1, 'Max Up-Time:', max_up_time)\n",
    "env.close()\n",
    "\n",
    "plt.plot(UpTime)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Question \n",
    "\n",
    "Implement deep Q-learning as described in the paper here:\n",
    "\n",
    "https://daiwk.github.io/assets/dqn.pdf\n",
    "\n",
    "In this paper, we have the states, and so there is no need to do the pre-processing described there.\n",
    "\n",
    "In my tests on this problem, it works substantially better than the SARSA  implementation \n",
    "with the following design choices:\n",
    "* Use the same Q-network architecture as used  in the SARSA algorithm\n",
    "* Same step size, discount factor, and learning rate as above\n",
    "* Mini-batch size of 20\n",
    "* Update the target network every 100 steps\n",
    "\n",
    "The deep Q-learning method can be implemented via a modification of the SARSA code above.\n",
    "\n",
    "You could probably make it work even better with further tuning.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Implement this code below and test it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
