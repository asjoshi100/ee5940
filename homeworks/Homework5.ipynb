{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Welcome\n",
    "\n",
    "Oh hey.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Markov Chains\n",
    "\n",
    "A Markov chain is a special case of a Markov decision process with no actions. Specifically, there is a transition distribution $p(S_{t+1}|S_t)$. Given an initial distribution, $P(S_0)$, we get a distribution over trajectories by:\n",
    "$$\n",
    "p(S_0,S_1,\\ldots,S_T) = p(S_0)p(S_1|S_0)\\cdots p(S_T|S_{T-1})\n",
    "$$\n",
    "\n",
    "Note that for any MDP and any policy, $\\pi$, there is an associated Markov chain defined by \n",
    "$$p_{\\pi}(s'|s) = \\sum_{a\\in\\mathcal{A}} p(s'|s,a)\\pi(a,|s)$$\n",
    "\n",
    "The transition probability of a Markov chain over a finite state space can be represented by a matrix:\n",
    "$$\n",
    "P(i,j) = p(s'=j|s=i).\n",
    "$$\n",
    "\n",
    "# Theory Question\n",
    "\n",
    "Let $S_0,S_1,S_2,\\ldots$ be a sequence of states generated by the Markov chain associated with $P\\in \\mathbb{R}^{n\\times n}$.\n",
    "\n",
    "Assume that the Markov chain always reaches a terminal state. Specifically, assume that there is a set $\\mathcal{S}_{\\mathrm{term}} \\subset \\{1,\\ldots,n\\}$ such that $p(\\hat s | \\hat s) = 1$ for all $\\hat s\\in \\mathcal{S}_{\\mathrm{term}}$ and there is an integer $H$ such that $p(S_t \\in \\mathcal{S}_{\\mathrm{term}}| S_0=i) > 0 $ for all $t\\ge H$ and all $i=1,\\ldots, n$.\n",
    "\n",
    "\n",
    "\n",
    "Without loss of generality, assume that $\\mathcal{S}_{\\mathrm{term}} = \\{m+1,\\ldots,n\\}$ for some $m<n$. In this case $P$ can be expressed as\n",
    "$$\n",
    "P = \\begin{bmatrix}\n",
    "Q & B \\\\\n",
    "0 & I\n",
    "\\end{bmatrix}.\n",
    "$$\n",
    "\n",
    "Show that for $i,j \\in \\{1,\\ldots,m\\}$ we have that $p(S_t = j| S_0=i)$ is the $(i,j)$ coordinate of $Q^t$.\n",
    "\n",
    "Write your solution here or attached a hand-written note."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Theory Question\n",
    "\n",
    "Show that there is a number $\\alpha \\in (0,1]$ such that $p(S_{kH}\\notin \\mathcal{S}_{\\mathrm{term}} | S_0=i)\\le (1-\\alpha)^k$ for all $i\\in \\{1,\\ldots,m\\}$.\n",
    "\n",
    "Hints:\n",
    "* First examine the $k=1$ case and use induction\n",
    "* In general we have $p(S_{kH}\\notin \\mathcal{S}_{\\mathrm{term}} | S_0=i)=\\sum_{j=1}^m p(S_{kH}=j | S_0=i)$\n",
    "\n",
    "Write your solution here or attached a hand-written note."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Theory Question\n",
    "\n",
    "Show that if $\\lambda$ is an eigenvalue of $Q$ then $|\\lambda| < 1$.\n",
    "\n",
    "Write your solution here or attached a hand-written note."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Theory Question\n",
    "\n",
    "Let $r$ be a vector in $\\mathbb{R}^n$.\n",
    "Show that if $r(i)=0$ and $v(i)=0$ for all $i\\in \\mathcal{S}_{\\mathrm{term}}$, the there are unique values $v(1),\\ldots,v(m)$ such that\n",
    "$$\n",
    "v = r + Pv\n",
    "$$\n",
    "\n",
    "Write your solution here or attached a hand-written note."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# OpenAI Gym\n",
    "\n",
    "Today we will be working with a simple problem called \"Frozen Lake\" from OpenAI Gym. The discussion from class of MDPs with terminal states was based on this example. It is a text-based maze environment that your controller will learn to navigate. It is slippery, however, so sometimes you don't always move where you try to go.\n",
    "\n",
    "You can install OpenAI Gym via\n",
    "\n",
    "```\n",
    "pip install gym\n",
    "```\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "import numpy as np\n",
    "import scipy.linalg as la\n",
    "\n",
    "# This loads the environment\n",
    "env = gym.make('FrozenLake-v0')\n",
    "# This shows the basic layout\n",
    "env.render()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The board can be interpreted as follows\n",
    "* S - Start State\n",
    "* F - Frozen parts\n",
    "* H - Holes\n",
    "* G - Goal\n",
    "\n",
    "The colored square corresponds to the current state.\n",
    "\n",
    "The episode ends if you hit a hole or the goal state. You recieve a reward of $1$ if you reach the goal and a reward of $0$ otherwise. \n",
    "\n",
    "Below is a  simulation of the system with randomly generated actions. It runs until the environment says it is `done`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s = env.reset()\n",
    "\n",
    "done = False\n",
    "R = []\n",
    "while not done:\n",
    "    a = env.action_space.sample()\n",
    "    s, r, done, info = env.step(a)\n",
    "    R.append(r)\n",
    "\n",
    "TimeLimitReached = False\n",
    "if 'TimeLimit.truncated' in info.keys():\n",
    "    if info['TimeLimit.truncated'] == True:\n",
    "        TimeLimitReached = True\n",
    "    \n",
    "env.render()\n",
    "print('Time Limit Reached?',TimeLimitReached)\n",
    "print('Sequence of Rewards:')\n",
    "print(R)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By default, the environment has a time limit of 100 steps. After this many steps, the environment will return a `done` value of `True`, even if the terminal state has not been reached. For example, if you try moving up the whole time, this will probably happen. This is important to keep in mind since:\n",
    "* If we were trying to determine the terminal states just based on simulation data, we should not mark non-terminal states as terminal. \n",
    "* If we are estimating the value obtained of a particular policy, if we count these terminated trajectories, we will get an inaccurate value because the trajectory should have continued past the time limit."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run this. It will probably reach the time limit.\n",
    "s = env.reset()\n",
    "\n",
    "done = False\n",
    "R = []\n",
    "while not done:\n",
    "    # Always try to move up\n",
    "    a = 3\n",
    "    s, r, done, info = env.step(a)\n",
    "    R.append(r)\n",
    "\n",
    "TimeLimitReached = False\n",
    "if 'TimeLimit.truncated' in info.keys():\n",
    "    if info['TimeLimit.truncated'] == True:\n",
    "        TimeLimitReached = True\n",
    "    \n",
    "env.render()\n",
    "print('Time Limit Reached?',TimeLimitReached)\n",
    "print('Sequence of Rewards:')\n",
    "print(R)\n",
    "print('The total steps is',len(R))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Extracting the Exact MDP\n",
    "\n",
    "For the simple text-based examples, we can extract MDP exactly. We can use this to test basic algorithms like policy iteration and value iteration. The code below computes `P_true`, `R_true`, `S_term`, and `S_non_term`.\n",
    "\n",
    "* `P_true[i,a,j]` encodes $p(S'=j|S=i,A=a)$\n",
    "* `R_true[i,a]` encodes $\\mathbb{E}[R_1 | S_0=i,A_0=a]$\n",
    "* `S_term` is a list of the terminal states\n",
    "* `S_non_term` is a list of the states that are not terminal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build it exactly\n",
    "env = gym.make('FrozenLake-v0')\n",
    "env.reset()\n",
    "\n",
    "nS = env.observation_space.n\n",
    "nA = env.action_space.n\n",
    "\n",
    "P_true = np.zeros((nS,nA,nS))\n",
    "R_true = np.zeros((nS,nA))\n",
    "S_term = []\n",
    "\n",
    "for s in range(nS):\n",
    "    for a in range(nA):\n",
    "        transitions = env.P[s][a]\n",
    "        for p,s_next,r,done in transitions:\n",
    "            P_true[s,a,s_next] += p\n",
    "            R_true[s,a] += p*r\n",
    "            \n",
    "            if done:\n",
    "                if s_next not in S_term:\n",
    "                    S_term.append(s_next)\n",
    "            \n",
    "            \n",
    "S_term = list(set(S_term))\n",
    "S_non_term = list(set(range(nS)) - set(S_term))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Coding Question\n",
    "\n",
    "Code up the policy iteration algorithm for the undiscounted case with terminal states. Compute an optimal policy, $\\pi^\\star$ and the optimal value function $V^\\star$. Display the values of $V^\\star$ so that I can check it. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Put your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Coding Question\n",
    "\n",
    "Run 1000 simulations using the optimal and display the average total return obtained by these simulations. It should be close to $V^*(0)$.\n",
    "\n",
    "Note, only include returns corresponding to simulations that did not reach the time limit in the average. If you include those in which the time limit is reached, you will bias the average."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Put your code here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
